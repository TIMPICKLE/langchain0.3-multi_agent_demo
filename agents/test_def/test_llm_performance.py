"""
LLMè¿æ¥å’Œæ€§èƒ½æµ‹è¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºæµ‹è¯•LLMæœåŠ¡çš„è¿æ¥æ€§èƒ½ï¼Œå¹¶å¸®åŠ©ä¼˜åŒ–å‚æ•°è®¾ç½®
"""
import sys
import os
import time
import requests

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.llm_config import LLMConfig

def test_basic_connection():
    """æµ‹è¯•åŸºæœ¬è¿æ¥"""
    print("ğŸ”— æµ‹è¯•åŸºæœ¬è¿æ¥...")
    
    config = LLMConfig()
    
    try:
        start_time = time.time()
        success = config.test_connection()
        end_time = time.time()
        
        if success:
            print(f"âœ… è¿æ¥æˆåŠŸ (è€—æ—¶: {end_time - start_time:.2f}ç§’)")
            return True
        else:
            print("âŒ è¿æ¥å¤±è´¥")
            return False
    except Exception as e:
        print(f"âŒ è¿æ¥å¼‚å¸¸: {e}")
        return False

def test_simple_request():
    """æµ‹è¯•ç®€å•è¯·æ±‚"""
    print("\nğŸ§ª æµ‹è¯•ç®€å•è¯·æ±‚...")
    
    config = LLMConfig()
    
    try:
        start_time = time.time()
        
        # æ„å»ºä¸€ä¸ªç®€å•çš„è¯·æ±‚
        messages = [
            {"role": "user", "content": "è¯·ç®€çŸ­å›ç­”ï¼šä½ å¥½"}
        ]
        
        response = config.call_llm(messages, max_tokens=50)
        end_time = time.time()
        
        print(f"âœ… è¯·æ±‚æˆåŠŸ (è€—æ—¶: {end_time - start_time:.2f}ç§’)")
        print(f"ğŸ“ å“åº”å†…å®¹: {response}")
        return True
        
    except Exception as e:
        end_time = time.time()
        print(f"âŒ è¯·æ±‚å¤±è´¥ (è€—æ—¶: {end_time - start_time:.2f}ç§’)")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        return False

def test_different_token_limits():
    """æµ‹è¯•ä¸åŒtokené™åˆ¶ä¸‹çš„æ€§èƒ½"""
    print("\nğŸš€ æµ‹è¯•ä¸åŒtokené™åˆ¶ä¸‹çš„æ€§èƒ½...")
    
    config = LLMConfig()
    token_limits = [50, 100, 200, 400, 800]
    
    test_message = [
        {"role": "user", "content": "è¯·ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹"}
    ]
    
    for max_tokens in token_limits:
        try:
            print(f"\nğŸ“Š æµ‹è¯• max_tokens={max_tokens}...")
            start_time = time.time()
            
            response = config.call_llm(test_message, max_tokens=max_tokens)
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            print(f"âœ… æˆåŠŸ (è€—æ—¶: {execution_time:.2f}ç§’)")
            print(f"ğŸ“ å“åº”é•¿åº¦: {len(response)}å­—ç¬¦")
            
            if execution_time > 30:
                print(f"âš ï¸  å“åº”æ—¶é—´è¾ƒé•¿: {execution_time:.2f}ç§’")
            
        except Exception as e:
            end_time = time.time()
            execution_time = end_time - start_time
            print(f"âŒ å¤±è´¥ (è€—æ—¶: {execution_time:.2f}ç§’): {e}")

def test_agent_specific_params():
    """æµ‹è¯•Agentä¸“ç”¨å‚æ•°"""
    print("\nğŸ¤– æµ‹è¯•Agentä¸“ç”¨å‚æ•°...")
    
    config = LLMConfig()
    agent_types = ["researcher", "planner", "executor", "reviewer"]
    
    test_message = [
        {"role": "user", "content": "è¯·ç®€è¦è¯´æ˜ä½ çš„ä½œç”¨"}
    ]
    
    for agent_type in agent_types:
        try:
            print(f"\nğŸ”§ æµ‹è¯• {agent_type} å‚æ•°...")
            start_time = time.time()
            
            response = config.call_llm(test_message, agent_type=agent_type)
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            print(f"âœ… æˆåŠŸ (è€—æ—¶: {execution_time:.2f}ç§’)")
            print(f"ğŸ“ å“åº”é¢„è§ˆ: {response[:100]}...")
            
        except Exception as e:
            end_time = time.time()
            execution_time = end_time - start_time
            print(f"âŒ å¤±è´¥ (è€—æ—¶: {execution_time:.2f}ç§’): {e}")

def test_concurrent_requests():
    """æµ‹è¯•å¹¶å‘è¯·æ±‚"""
    print("\nâš¡ æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½...")
    
    import asyncio
    import aiohttp
    
    async def make_request(session, config, request_id):
        """å‘é€å•ä¸ªå¼‚æ­¥è¯·æ±‚"""
        try:
            messages = [
                {"role": "user", "content": f"è¯·ç®€çŸ­å›ç­”é—®é¢˜{request_id}: 1+1ç­‰äºå¤šå°‘ï¼Ÿ"}
            ]
            
            request_data = config.build_request_data(messages, "researcher")
            
            start_time = time.time()
            async with session.post(
                config.get_chat_url(),
                headers=config.get_headers(),
                json=request_data,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                result = await response.json()
                end_time = time.time()
                
                if response.status == 200:
                    content = result['choices'][0]['message']['content']
                    return {
                        "id": request_id,
                        "success": True,
                        "time": end_time - start_time,
                        "content": content[:50]
                    }
                else:
                    return {
                        "id": request_id,
                        "success": False,
                        "time": end_time - start_time,
                        "error": f"HTTP {response.status}"
                    }
        except Exception as e:
            return {
                "id": request_id,
                "success": False,
                "time": time.time() - start_time,
                "error": str(e)
            }
    
    async def run_concurrent_test():
        config = LLMConfig()
        
        async with aiohttp.ClientSession() as session:
            # å¹¶å‘3ä¸ªè¯·æ±‚
            tasks = [make_request(session, config, i) for i in range(3)]
            
            start_time = time.time()
            results = await asyncio.gather(*tasks)
            end_time = time.time()
            
            print(f"ğŸ å¹¶å‘æµ‹è¯•å®Œæˆ (æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’)")
            
            success_count = sum(1 for r in results if r["success"])
            print(f"ğŸ“Š æˆåŠŸç‡: {success_count}/{len(results)}")
            
            for result in results:
                status = "âœ…" if result["success"] else "âŒ"
                print(f"{status} è¯·æ±‚{result['id']}: {result['time']:.2f}ç§’")
                if result["success"]:
                    print(f"   å“åº”: {result['content']}...")
                else:
                    print(f"   é”™è¯¯: {result['error']}")
    
    try:
        asyncio.run(run_concurrent_test())
    except Exception as e:
        print(f"âŒ å¹¶å‘æµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª LLMè¿æ¥å’Œæ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # åŸºæœ¬è¿æ¥æµ‹è¯•
    if not test_basic_connection():
        print("âŒ åŸºæœ¬è¿æ¥å¤±è´¥ï¼Œè·³è¿‡åç»­æµ‹è¯•")
        return
    
    # ç®€å•è¯·æ±‚æµ‹è¯•
    if not test_simple_request():
        print("âŒ ç®€å•è¯·æ±‚å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥æœåŠ¡çŠ¶æ€")
        return
    
    # æ€§èƒ½æµ‹è¯•
    test_different_token_limits()
    
    # Agentå‚æ•°æµ‹è¯•
    test_agent_specific_params()
    
    # å¹¶å‘æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
    try:
        import aiohttp
        test_concurrent_requests()
    except ImportError:
        print("\nâš ï¸  è·³è¿‡å¹¶å‘æµ‹è¯• (éœ€è¦å®‰è£… aiohttp: pip install aiohttp)")
    
    print("\n" + "=" * 50)
    print("ğŸ¯ æµ‹è¯•å»ºè®®:")
    print("1. å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼ŒLLMæœåŠ¡å·¥ä½œæ­£å¸¸")
    print("2. å¦‚æœå“åº”æ—¶é—´è¶…è¿‡30ç§’ï¼Œè€ƒè™‘é™ä½max_tokens")
    print("3. å¦‚æœç»å¸¸è¶…æ—¶ï¼Œæ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒæœåŠ¡è´Ÿè½½")
    print("4. å»ºè®®ä½¿ç”¨è¾ƒå°çš„max_tokenså€¼ä»¥æé«˜å“åº”é€Ÿåº¦")

if __name__ == "__main__":
    main()
